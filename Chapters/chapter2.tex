\chapter{EKF-SLAM Implementation}
\label{ch:chapter2}
In Chapter~\ref{ch:chapter1} the EKF-SLAM algorithm in the context of SLAM was explained. As mentioned in Section~\ref{sec:chapter1:kf} and Section~\ref{subsec:chapter1:kf:ekf}, the algorithm can be summarized in two steps: prediction and correction. In the first step, prediction about the next state of the system is done, while during correction, this estimation is updated. \\

In this chapter, an EKF-SLAM implementation is shown, starting from the used drone characteristics, going through the prediction and correction steps, and ending with the overall system's architecture.

\section{The Drone}
\label{sec:chapter2:drone}
\subsection{Characteristics}
\label{subsec:chapter2:drone:characteristics}
The used drone has a common characteristics: four rotors disposed as an X. The drone frame is called 3DR Iris Quadrotor, and can be seen in Figure~\ref{fig:chapter2:drone:frame}. Its total weight is XX kg., and it diameter is XX cm.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/fig4-quad-frame.png}
    \caption{3DR Iris Quadrotor frame.}
    \label{fig:chapter2:drone:frame}
\end{figure}

\subsubsection{Flight Controller}
The flight controller used in this case is a PixHawk 4, and its characteristics are shown in Table~\ref{tab:chapter2:drone:px4fc}. Worth mentioning that it has integrated an accelerometer, a gyroscope, a magnetometer and a barometer.

\begin{table}[h]
    \centering
    \begin{tabular}{cp{27em}}
        \toprule \textsc{Item} & \textsc{Description}\\
        \midrule
        \multirow{2}{*}{Main FMU processor} & STM32F765 \\
        & 32 Bit Arm® Cortex®-M7, 216MHz, 2MB memory, 512KB RAM \\
        \midrule
        \multirow{2}{*}{IO Processor} & STM32F100 \\
        & 32 Bit Arm® Cortex®-M3, 24MHz, 8KB SRAM \\
        \midrule
        \multirow{4}{*}{On-board sensors} & Accel/Gyro: ICM-20689 \\
        & Accel/Gyro: BMI055 \\
        & Magnetometer: IST8310 \\
        & Barometer: MS5611 \\
        \midrule
        GPS & ublox Neo-M8N GPS/GLONASS receiver; integrated magnetometer IST8310 \\
        \midrule
        \multirow{10}{*}{Interfaces} & 8-16 PWM outputs (8 from IO, 8 from FMU) \\
        & 3 dedicated PWM/Capture inputs on FMU \\
        & Dedicated R/C input for CPPM \\
        & Dedicated R/C input for Spektrum / DSM and S.Bus with analog / PWM RSSI input \\
        & Dedicated S.Bus servo output \\
        & 5 general purpose serial ports \\
        & 3 I2C ports \\
        & 4 SPI buses \\
        & Up to 2 CANBuses for dual CAN with serial ESC \\
        & Analog inputs for voltage / current of 2 batteries \\
        \midrule
        \multirow{3}{*}{Power System} & Power module output: 4.9~5.5V \\
        & USB Power Input: 4.75~5.25V \\
        & Servo Rail Input: 0~36V \\
        \midrule
        \multirow{2}{*}{Weight and Dimensions} & Weight: 15.8g \\
        & Dimensions: 44x84x12mm \\
        \midrule
        Operating temperature & -40 to 85°c \\
        \bottomrule
    \end{tabular}
    \caption[PixHawk 4 Specification]{Technical specification of the PixHawk 4 flight controller.}
    \label{tab:chapter2:drone:px4fc}
\end{table}

\subsubsection{Additional Sensors}
The drone was equipped with several sensors that are the input for localization, mapping and path planning algorithms, among others.\\

The localization and mapping algorithm makes use, in an indirect way, of monocular and stereo cameras, and laser range sensors. Four cameras were mounted in order to be able to look around (every 90 degrees). Two stereo cameras were mounted, one points forward in order to update the Octomap and the other points downwards in order to see the markers; Also, both of them are used to build a 3-dimensional map, used for obstacle avoidance and with the height estimation algorithm. Furthermore, eight laser range sensors were mounted every 45 degrees, and one laser range sensor was mounted pointing downwards in order to estimate the current drone height.\\

\subsection{Reference Frames}
\label{subsec:chapter2:drone:frames}
The reference frames in the system can be seen in Figure~\ref{fig:chapter2:drone:frames:frames}, and it can be seen three frames: \emph{map}, \emph{odom} and \emph{base\_link}. The \emph{map} frame, also called \emph{world} or \emph{global} frame, is the static reference frame, where the global drone's position and global markers' position is set. The odom reference frame is similar to map, but with the difference that this frame drifts with time, as happens with the pure odometry. Finally, the base\_link frame, also referred as \emph{body} frame, refers to the center of mass of the drone. All these transformations are published by different nodes: the map to odom transform is handled by the \emph{rtabmap} node, the odom to base\_link is handled by \emph{mavros} node. There are other transformations in the system, mainly related to the base\_link reference frame and the different cameras and sensors in the drone. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/fig5-frames.png}
    \caption{Main reference frames in the system.}
    \label{fig:chapter2:drone:frames:frames}
\end{figure}

Finally, there is a transformation that worth mentioning: the \ac{NED} to \ac{ENU} transform. As mentioned in Section~\ref{sec:chapter1:ros}, ROS uses the ENU convention, while the convention adopted by the Aerospace community is the NED. Also, as mentioned in Section~\ref{sssec:chapter2:drone:mavlink}, MAVROS is in charge of doing and publish this transformation. In Figure~\ref{fig:chapter2:drone:frames:enu2ned} a simple scheme of the transformation can be seen. As explained before, this transform consists on rotating the X-axis and the Y-axis by 90 degrees, hence the homogeneous rotation matrix will have the following form:
\begin{align}
    R_{ENU}^{NED} & = \begin{bmatrix}
        0 & -1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        -1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1
    \end{bmatrix}
\end{align}


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/fig6-ned2enu.png}
    \caption{NED to ENU conversion scheme. \cite{mavros}}
    \label{fig:chapter2:drone:frames:enu2ned}
\end{figure}

\section{Prediction}
\label{sec:chapter2:prediction}
During the prediction step, the motion model and the covariance matrix update are computed. The motion model will update the state vector, which will store the position X, Y and Z in the global reference frame, and the orientation in the Z-axis of the drone. Fortunately MAVROS provides a node that makes odometry estimation based on different sensors outputs. The messages published by the odometry node, of type \inlinesrc{nav_msgs::Odometry}, provide the linear velocity, the angular velocity and pose information. The velocity information is used to estimate the position in X, Y and Z, and the drone's orientation along the Z-axis. However, the velocity estimation provided by MAVROS is relative to the body reference frame, and this has to be transformed into the world reference frame, so before estimating the global position of the drone it is mandatory to do this transformation.

\begin{align}
    u &= \begin{bmatrix} v_x^b & v_y^b & v_z^b & \omega_x^b & \omega_y^b & \omega_z^b & \phi^b & \theta^b & \psi^b \end{bmatrix}^T\\
    v^w &= \textbf{T} * \begin{bmatrix} v_x^b \\ v_y^b \\ v_z^b \end{bmatrix}
\end{align}
where the control vector $u$ is composed by the linear velocities in the body reference frame ($v_x^b$, $v_y^b$, $v_z^b$), the angular velocities in the body reference frame ($\omega_x^b$, $\omega_y^b$, $\omega_z^b$) and the drone's orientation ($\phi^b$, $\theta^b$, $\psi^b$). Additionally, $\textbf{T}$ is the homogeneous transform (see Section~\ref{sec:chapter1:transform}) using the current orientation of the drone: $\phi^b$, $\theta^b$ and $\mu_{\psi}$. Notice the third element of the orientation ($\mu_{\psi}$), this is because the orientation over the Z-axis is estimated by the filter. As result, $v^w$ will be a column vector with the linear velocities in the global reference frame. \\

Given this, the motion model update can be summarized in the following calculation:
\begin{align}
    \hat\mu &=
    \begin{bmatrix}
        \mu_{t-1, x^w} \\ \mu_{t-1, y^w} \\ \mu_{t-1, z^w}
    \end{bmatrix}
    + \Delta t * v^w \\
    \hat\mu_{\psi} &= \mu_{t-1, \psi} + \Delta t * \omega_z^b
\end{align}

Then, $\textbf{G}_t$, which is the Jacobian matrix of the motion model, should be computed. As mentioned in Section~\ref{subsec:chapter1:slam:ekfslam}, it has the following characteristic:
\begin{align}
    G_t &= \begin{bmatrix}
        G_r & \textbf{0} \\
        \textbf{0} & \textbf{I}
    \end{bmatrix}\\
    G_r &= \begin{bmatrix}
        1 & 0 & 0 & G_{r, 14} \\
        0 & 1 & 0 & G_{r, 24} \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{bmatrix}
\end{align}
\begin{align*}
     G_{r, 14} &= -\Delta t * (v_y^w * (c_{\phi} * c_{\psi} + s_{\theta} * s_{\phi} * s_{\psi}) - v_z^w * (c_{\psi} * s_{\phi} - c_{\phi} * s_{\theta} * s_{\psi}) + v_x^w * c_{\theta} * s_{\psi}) \\
     G_{r, 24} &= -\Delta t * (v_z^w * (c_{\psi} * s_{\phi} + c_{\phi} * s_{\theta} * c_{\psi}) -v_y^w * (c_{\phi} * s_{\psi} - s_{\theta} * s_{\phi} * c_{\psi}) + v_x^w * c_{\theta} * s_{\psi})
\end{align*}
where
\begin{alignat*}{3}
    c_{\phi} &= \cos{\left(u_{\phi}\right)}, \quad & c_{\theta} &= \cos{\left(u_{\theta}\right)}, \quad & c_{\psi} &= \cos{\left(\mu_{\psi}\right)} \\
    s_{\phi} &= \sin{\left(u_{\phi}\right)}, \quad & s_{\theta} &= \sin{\left(u_{\theta}\right)}, \quad & s_{\psi} &= \sin{\left(\mu_{\psi}\right)}
\end{alignat*}

The function $g$ that models the motion of the drone is assumed to be perfect and therefore noise free. So, before computing the covariance update, it is necessary to compute the process noise covariance matrix ($R_t$) that encodes the motion model noise which, in this case, is related to the underlying dynamics of the drone flight. The noise is assumed to be additive and Gaussian, and therefore, the motion model can be decomposed as:
\begin{equation}
    x_t = g(u_t, x_{t-1}) + \mathcal{N}\left(0, R_t\right)
    \label{eq:chapter2:prediction:motion_w_noise}
\end{equation}
\begin{equation}
    R_t = N * U * N^t
    \label{eq:chapter2:prediction:control_cov}
\end{equation}
the noise part in equation (\ref{eq:chapter2:prediction:motion_w_noise}) relates to the acceleration component that is, in this case, unknown. However, it is known from a theoretical perspective: the acceleration component in an accelerated movement is $\frac{1}{2}\Delta t^2 a$, where $a$ is the body's acceleration. Given this, we can assume that the noise component in the motion model is:
\begin{equation}
    \mathcal{N}\left(0, R_t\right) = \frac{1}{2} \Delta t^2 T_w^b \begin{bmatrix}
        a_x \\ a_y \\ a_z \\ a_{\psi}
    \end{bmatrix}
\end{equation}
where $T_w^b$ is the transformation matrix from body to world reference frame. The covariance of the process noise ($R_t$) can be decompose as shown in equation~(\ref{eq:chapter2:prediction:control_cov}), where matrix $N$ is the Jacobian of acceleration term  with respect to the state vector, and matrix $U$ is the estimated average acceleration.
\begin{equation}
    N = \frac{\partial \frac{1}{2} \Delta t^2 T_w^b \textbf{a}}{\partial \mu}
\end{equation}
\begin{equation}
    U = \textbf{I} * \begin{bmatrix}
        a_{avg, x} \\ a_{avg, y} \\ a_{avg, z} \\ a_{avg, \psi}
    \end{bmatrix}
\end{equation}
The multiplication in (\ref{eq:chapter2:prediction:control_cov}) provides an approximate mapping between the motion noise in control space and the motion noise in the state space.\\

Finally, the covariance update should be computed as follow
\begin{align}
    \hat\Sigma &= G_t * \Sigma * G_t^T + R_t
\end{align}

\section{Correction}
\label{sec:chapter2:correction}
While the drone is moving around the environment it senses different landmarks that may be included or not in the state vector. These observations will eventually improve the localization of the drone, and will improve the landmarks' poses if needed. The whole process involves the computation of the Jacobian of the observation model for the seen landmark, the computation of the Kalman gain, and the update of the state vector and covariance matrix.\\

To perform the correction step, EKF-SLAM needs a linearized observation model with additive Gaussian noise. In the case studied in this work, there are two kinds of landmarks and, therefore, two different observation models. The main difference between these two type of landmarks, is that the position of poles type is known, while it is not known in the case of markers. Hence, every time the robot "sees" a pole, the algorithm will update the drone's pose and the known markers' pose; while every time it "sees" a marker two course of action are possible:
\begin{enumerate}[a)]
    \item{if the marker is not known, it is added to the state vector, enlarging it along with the covariance matrix.}
    \item{if the marker is known, its pose, the pose of all other markers, and the drone's pose are updated.}
\end{enumerate}

The observation model is, as with the motion model in equation~(\ref{eq:chapter2:prediction:motion_w_noise}), assumed to be perfect and with an additive Gaussian noise. The noise here is related to the observation process, and so, related to the used sensors.
\begin{equation}
    z_i = h_i\left(x_t\right) + \mathcal{N}\left(0, Q_t\right)
    \label{eq:chapter2:correction:obs_w_noise}
\end{equation}
Consequently, the noise covariance matrix of the observation model cannot be deducted as with the noise covariance matrix of the motion model. In this case, the matrix should be constructed empirically based on the sensors' characteristics, and it has the following characteristic:
\begin{equation}
    Q_t = \begin{bmatrix}
        \sigma_1^2 &  & \textbf{0} \\
         & \ddots & \\
        \textbf{0} & & \sigma_n^2 \\
    \end{bmatrix}
\end{equation}
where the diagonal elements $\sigma_{1..n}$ are the standard deviation of the sensor. Depending on the sensor used, the diagonal elements can be the standard deviation for the range and bearing components (distance, azimuth and elevation), or others.\\

As shown in Algorithm~\ref{alg:chapter1:slam:ekfslam} several steps are followed during the correction part of the algorithm. After computing the observation model and its Jacobian matrix, the Kalman gain and the innovation should be calculated, and finally, the state vector and covariance matrix updates should be done.

\subsection{Observation model for Poles}
\label{subsec:chapter2:correction:poles}
In the case of Poles, a range and bearing method is used. In this case, since the poles have a known position, their information is not kept in the state vector and therefore, this information will be used for localization purposes.\\

A ROS node will publish the range and bearing information every time the drone sees a pole, and this information will be used to calculate the innovation based on the predicted range and bearing. Hence, the observation model used for poles is computed in the following way:
\begin{equation}
    \begin{bmatrix}
        p_{i, x}^b \\ p_{i, y}^b \\ p_{i, z}^b
    \end{bmatrix} = \textbf{T}^{-1} \begin{bmatrix}
        p_{i, x}^w \\ p_{i, y}^w \\ p_{i, z}^w
\end{bmatrix}
\label{eq:chapter2:correction:pole:world2body_transform}
\end{equation}
\begin{equation}
    h_i(\hat\mu_t) = \begin{bmatrix}
        p_{i, \rho} \\ p_{i, \alpha} \\ p_{i, \beta}
    \end{bmatrix} = \begin{bmatrix}
    \sqrt{ p_{j, x^b}^2 + p_{j, y^b}^2 } \\
    \atantwo{p_{j, y}^b}{p_{j, x}^b} \\
    \atantwo{p_{j, z}^b}{p_{j,\rho}^b}
\end{bmatrix}
\label{eq:chapter2:correction:pole:range_bearing}
\end{equation}

In equation~(\ref{eq:chapter2:correction:pole:world2body_transform}), $\textbf{T}^{-1}$ corresponds to the inverse of the homogeneous transformation matrix with respect to the current drone's pose, and elements $p_i^w$ are the $x$, $y$ and $z$ coordinates of the $i$ pole's tip in the world reference frame. This way, the global position of the pole $i$ is projected to the body reference frame. After that, the range ($\rho$), azimuth angle ($\alpha$) and elevation angle ($\beta$) are calculated, as shown in equation~(\ref{eq:chapter2:correction:pole:range_bearing}). In Figure~\ref{fig:chapter2:correction:poles:range_bearing} an example of the range and bearing is shown.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/fig7-range_and_bearing}
    \caption[Range and Bearing example]{Range and bearing example. The drone sees a pole, process the data from the sensors and estimates the distance ($\rho$), the elevation angle ($\beta$), and the azimuth angle ($\alpha$). The elevation angle is calculated based on the top extreme of the pole.}
    \label{fig:chapter2:correction:poles:range_bearing}
\end{figure}

Seen a pole affects only the drone's pose, and therefore, the Jacobian matrix of the observation model will have the following form:
\begin{equation}
    H_i = \begin{bmatrix}
        \frac{\partial \rho'}{\partial \mu_x} & \frac{\partial \rho'}{\partial \mu_y} & \frac{\partial \rho'}{\partial \mu_z} & \frac{\partial \rho'}{\partial \mu_{\psi}} & \dots & 0 \\
        \frac{\partial \alpha'}{\partial \mu_x} & \frac{\partial \alpha'}{\partial \mu_y} & \frac{\partial \alpha'}{\partial \mu_z} & \frac{\partial \alpha'}{\partial \mu_{\psi}} & \dots & 0 \\
        \frac{\partial \beta'}{\partial \mu_x} & \frac{\partial \beta'}{\partial \mu_y} & \frac{\partial \beta'}{\partial \mu_z} & \frac{\partial \beta'}{\partial \mu_{\psi}} & \dots & 0
    \end{bmatrix}
\end{equation}
where $\rho'$ corresponds to the distance part of the observation model, $\alpha'$ is the azimuth part, and $ \beta'$ the elevation part. The elements after the \nth{4} column are all 0, which means, as said before, that the observation of a pole will not affect the pose of the markers.

\subsection{Observation model for Markers}
\label{subsec:chapter2:correction:markers}
The observation model for the markers is a bit different. In this case a ROS node is responsible of detecting, tracking and publish the pose of the markers with respect to the camera that has seen it. The ROS package responsible of this process is called \inlinesrc{visp_auto_tracker}, and publishes messages of type \inlinesrc{geometry_msgs::PoseStamped}, which provides the position and orientation of the seen marker. An example of this situation can be seen in Figure~\ref{}\\

Every time the camera that points down sees a known marker, the \inlinesrc{visp_auto_tracker} node will publish the pose of that marker. This published pose is with respect to the camera which is not positioned at the drone's center of mass and so, a different transformation is needed. This process can be seen in equation~(\ref{eq:chapter2:correction:markers:world2body_trasnform}), and compromises two transformations: the drone's pose from world to camera frame, and from camera to body reference frame.
\begin{equation}
    \begin{bmatrix}
        c_x^w \\ c_y^w \\ c_z^w \\ c_{\phi}^w \\ c_{\theta}^w \\ c_{\psi}^w
    \end{bmatrix} = (\textbf{T}_r^w * \textbf{T}_c^r)^{-1} * \mathbf{T_c}_w^c
    \label{eq:chapter2:correction:markers:world2body_trasnform}
\end{equation}
where, $\textbf{T}_r^w$ is the homogeneous transformation matrix of the drone's pose, $\textbf{T}_c^r$ is the homogeneous transformation matrix of the camera's pose, and $\mathbf{T_c}_w^c$ is the homogeneous transformation matrix of the seen marker's pose which transforms its pose from camera reference frame to world reference frame.\\

As mentioned before, seen a marker will update the drone's pose and that marker's pose, and therefore the Jacobian matrix of the observation model will have a different aspect from the pole's case. Here, the Jacobian can be split in two parts as shown in equation~(\ref{eq:chater2:correction:markers:jacobian}), where the left part, as with the poles, affects the drone's pose, while the right part of the matrix affects the seen marker's pose.
\begin{align}
    H_i &= \begin{bmatrix}
        \frac{\partial h_i(\hat\mu)}{\partial \mu_r} & \dots & \frac{\partial h_i(\hat\mu)}{\partial \mu_{m_i}} & \dots
    \end{bmatrix}
    \label{eq:chater2:correction:markers:jacobian}\\
    \frac{\partial h_i(\hat\mu)}{\partial \mu_r} &= \begin{bmatrix}
        \frac{\partial x_m'}{\partial \mu_x} & \frac{\partial x_m'}{\partial \mu_y} & \frac{\partial x_m'}{\partial \mu_z} & \frac{\partial x_m'}{\partial \mu_{\psi}}\\
        \frac{\partial y_m'}{\partial \mu_x} & \frac{\partial y_m'}{\partial \mu_y} & \frac{\partial y_m'}{\partial \mu_z} & \frac{\partial y_m'}{\partial \mu_{\psi}}\\
        \frac{\partial z_m'}{\partial \mu_x} & \frac{\partial z_m'}{\partial \mu_y} & \frac{\partial z_m'}{\partial \mu_z} & \frac{\partial z_m'}{\partial \mu_{\psi}}\\
        \frac{\partial \phi_m'}{\partial \mu_x} & \frac{\partial \phi_m'}{\partial \mu_y} & \frac{\partial \phi_m'}{\partial \mu_z} & \frac{\partial \phi_m'}{\partial \mu_{\psi}}\\
        \frac{\partial \theta_m'}{\partial \mu_x} & \frac{\partial \theta_m'}{\partial \mu_y} & \frac{\partial \theta_m'}{\partial \mu_z} & \frac{\partial \theta_m'}{\partial \mu_{\psi}}\\
        \frac{\partial \psi_m'}{\partial \mu_x} & \frac{\partial \psi_m'}{\partial \mu_y} & \frac{\partial \psi_m'}{\partial \mu_z} & \frac{\partial \psi_m'}{\partial \mu_{\psi}}
    \end{bmatrix}
    \label{eq:chater2:correction:markers:jacobian_wrt_state}\\
    \frac{\partial h_i(\hat\mu)}{\partial \mu_m} &= \begin{bmatrix}
        \frac{\partial x_m'}{\partial \mu_{m_i, x}} & \frac{\partial x_m'}{\partial \mu_{m_i, y}} & \frac{\partial x_m'}{\partial \mu_{m_i, z}} & \frac{\partial x_m'}{\partial \mu_{m_i, \phi}} & \frac{\partial x_m'}{\partial \mu_{m_i, \theta}} & & \frac{\partial x_m'}{\partial \mu_{m_i, \psi}}\\
        \frac{\partial y_m'}{\partial \mu_{m_i, x}} & \frac{\partial y_m'}{\partial \mu_{m_i, y}} & \frac{\partial y_m'}{\partial \mu_{m_i, z}} & \frac{\partial y_m'}{\partial \mu_{m_i, \phi}} & \frac{\partial y_m'}{\partial \mu_{m_i, \theta}} & & \frac{\partial y_m'}{\partial \mu_{m_i, \psi}}\\
        \frac{\partial z_m'}{\partial \mu_{m_i, x}} & \frac{\partial z_m'}{\partial \mu_{m_i, y}} & \frac{\partial z_m'}{\partial \mu_{m_i, z}} & \frac{\partial z_m'}{\partial \mu_{m_i, \phi}} & \frac{\partial z_m'}{\partial \mu_{m_i, \theta}} & & \frac{\partial z_m'}{\partial \mu_{m_i, \psi}}\\
        \frac{\partial \phi_m'}{\partial \mu_{m_i, x}} & \frac{\partial \phi_m'}{\partial \mu_{m_i, y}} & \frac{\partial \phi_m'}{\partial \mu_{m_i, z}} & \frac{\partial \phi_m'}{\partial \mu_{m_i, \phi}} & \frac{\partial \phi_m'}{\partial \mu_{m_i, \theta}} & & \frac{\partial \phi_m'}{\partial \mu_{m_i, \psi}}\\
        \frac{\partial \theta_m'}{\partial \mu_{m_i, x}} & \frac{\partial \theta_m'}{\partial \mu_{m_i, y}} & \frac{\partial \theta_m'}{\partial \mu_{m_i, z}} & \frac{\partial \theta_m'}{\partial \mu_{m_i, \phi}} & \frac{\partial \theta_m'}{\partial \mu_{m_i, \theta}} & & \frac{\partial \theta_m'}{\partial \mu_{m_i, \psi}}\\
        \frac{\partial \psi_m'}{\partial \mu_{m_i, x}} & \frac{\partial \psi_m'}{\partial \mu_{m_i, y}} & \frac{\partial \psi_m'}{\partial \mu_{m_i, z}} & \frac{\partial \psi_m'}{\partial \mu_{m_i, \phi}} & \frac{\partial \psi_m'}{\partial \mu_{m_i, \theta}} & & \frac{\partial \psi_m'}{\partial \mu_{m_i, \psi}}
    \end{bmatrix}
    \label{eq:chater2:correction:markers:jacobian_wrt_marker}
\end{align}
These two matrices may look scary at first sight, however, half of their elements are 0 as can be seen in the Appendix~\ref{appendixa}.\\

Furthermore, unlike the case of poles, the markers' pose is unknown the first time, and therefore the algorithm will introduce their pose into the state vector when the drone sees a previously unknown marker.
\subsubsection{Adding new Markers}
\label{subsubsec:chapter2:correction:markers:add}

\section{Normalized Estimation Error Squared}
\label{sec:chapter2:nees}

\section{Overall Architecture}
\label{sec:chapter2:arch}
\subsection{ROS nodes}
\label{subsec:chapter2:arch:nodes}
