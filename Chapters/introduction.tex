\chapter{Introduction}
\label{introduction}

Network connected devices such as personal computers, mobile phones, or gaming consoles are nowadays enjoying immense popularity. In parallel, the Web and the humongous amount of services it offers have certainly became the most ubiquitous tools of all the times. \textsf{Facebook} counts more than 250 millions active users of which 65 millions are using it on mobile devices; not to mention that more than 1 billion photos are uploaded to the site \emph{each   month}~\citep{facebook-stats}. And this is just one, popular website. One year ago, \textsf{Google} estimated that the approximate number of unique \acp{URL}\index{URL} is 1 trillion~\citep{google-is-big}, while \texttt{YouTube} has stocked more than 70 million videos as of March 2008, with 112,486,327 views just on the most popular video as of January 2009~\citep{social-media-stats}. And people from all over the world inundate the Web with more than 3 million tweets \emph{per day}. Not only the Web 2.0 has became predominant; in fact, thinking that on December 1990 the Internet was made of \emph{one} site and today it counts more than 100 million sites is just astonishing~\citep{internet-timeline}.

The Internet and the Web are huge~\citep{inetworldstats}. The relevant fact, however, is that they both became the most advanced workplace. Almost every industry connected its own network to the Internet and relies on these infrastructures for a vast majority of transactions; most of the time monetary transactions. As an example, every year \textsf{Google} looses approximately 110 millions of US Dollars in ignored ads because of the \emph{``I'm feeling lucky''} button. The scary part is that, during their daily work activities, people typically pay poor or no attention at all to the risks that derive from exchanging any kind of information over such a complex, interconnected infrastructure. This is demonstrated by the effectiveness of social engineering~\citep{deception} scams carried over the Internet or the phone~\citep{social-engineering-fundamentals}. Recall that 76\% of the phishing is related to finance. Now, compare this landscape to what the most famous security quote states.

\begin{quotation}
  ``The only truly secure computer is one buried in concrete, with the   power turned off and the network cable cut''.
   ---\emph{Anonymous}
\end{quotation}

In fact, the Internet is all but a safe place~\citep{whid}, with more than 1,250 \emph{known} data breaches between 2005 and 2009 \citep{data-breaches-chronology} and an estimate of 263,470,869 records stolen by intruders. One may wonder why the advance of research in computer security and the increased awareness of governments and public institutions are still not capable of avoiding such incidents. Besides the fact that the aforementioned numbers would be order of magnitude higher in absence of countermeasures, todays' security issues are, basically, caused by the combination of two phenomena: the high amount of software vulnerabilities and the effectiveness of todays' exploitation strategy.

\begin{description}
\item[software flaws] --- (un)surprisingly, software is affected by   vulnerabilities. Incidentally, tools that have to do with the Web,   namely, browsers and 3\textsuperscript{rd}-party extensions, and web   applications, are the most vulnerable ones. For instance, in 2008,   \textsf{Secunia} reported around 115 security vulnerabilities for   \textsf{Mozilla Firefox}, 366 for \textsf{Internet Explorer}'s   \textsf{ActiveX}~\citep{secunia2008}. Office suites and e-mail   clients, that are certainly the must-have-installed tool   on every workstation, hold the second position~\citep{sans20}.
  
\item[massification of attacks] --- in parallel to the explosion of   the Web 2.0, attackers and the underground economy have quickly   learned that a sweep of exploits run against \emph{every} reachable   host have more chances to find a vulnerable target and, thus, is   much more profitable compared to a single effort to break into a   high-value, well-protected machine.
\end{description}

These circumstances have initiated a vicious circle that provides the attackers with a very large pool of vulnerable targets. Vulnerable client hosts are compromised to ensure virtually unlimited bandwidth and computational resources to attackers, while server side applications are violated to host malicious code used to infect client visitors. And so forth. An old fashioned attacker would have violated a single site using all the resources available, stolen data and sold it to the underground market. Instead, a modern attacker adopts a ``vampire'' approach and exploit client-side software vulnerabilities to take (remote) control of million hosts. In the past the diffusion of malicious code such as viruses was sustained by sharing of infected, cracked software through floppy or compact disks; nowadays, the Web offers unlimited, public storage to attackers that deploy their exploit on compromised websites.

Thus, not only the type of vulnerabilities has changed, posing virtually every interconnected device at risk. The exploitation strategy created new types of threats that take advantage of classic malicious code patterns but in a new, extensive, and tremendously effective way.

\section{Todays' Security Threats}
\label{introduction:motivation} Every year, new threats are discovered and attacker take advantage of them until effective countermeasures are found. Then, new threats are discovered, and so forth. \textsf{Symantec} quantifies the amount of new malicious code threats to be 1,656,227 as of 2008 \citep{symantec_threat_report_2009}, 624,267 one year earlier and only 20,547 in 2002. Thus, countermeasures must advance at least with the same grow rate. In addition:

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{Figures/bots.pdf}
  \caption{Illustration taken from~\citep{holz} and \copyright 2005 IEEE. Authorized license limited to \institution.}
  \label{fig:bots}
\end{figure}

\begin{quotation}
  [...] the current threat landscape --- such as the increasing complexity and sophistication of attacks, the evolution of attackers
and attack patterns, and malicious activities being pushed to emerging countries --- show not just the benefits of, but also the need for increased cooperation among security companies, governments, academics, and other organizations and individuals to combat these changes~\citep{symantec_threat_report_2009}.
\end{quotation}

Todays' underground economy run a very proficient market: everyone can buy credit card information for as low as \$0.06--\$30, full identities for just \$0.70--\$60 or rent a scam hosting solution for \$3--\$40 per week plus \$2-\$20 for the design~\citep{symantec_threat_report_2009}.

The main underlying technology actually employs a classic type of software called \emph{bot} (jargon for \emph{robot}), which is not malicious \emph{per s\'e}, but is used to remotely control a network of compromised hosts, called \emph{botnet}~\citep{holz}. Remote commands can be of any type and typically include launching an attack, starting a phishing or spam campaign, or even updating to the latest version of the bot software by downloading the binary code from a host controlled by the attackers (usually called \emph{bot master})~\citep{torpig}. The exchange good has now become the botnet infrastructure itself rather than the data that can be stolen or the spam that can be sent. These are mere outputs of todays' most popular service offered for rent by the underground economy.

\subsection{The Role of Intrusion Detection}
\label{introduction:motivation:ids-role}
The aforementioned, dramatic big picture may lead to think that the malicious software will eventually proliferate at every host of the Internet and no effective remediation exists. However, a more careful analysis reveals that, despite the complexity of this scenario, the problems that must be solved by a security infrastructure can be decomposed into relatively simple tasks that, surprisingly, may already have a solution. Let us look at an example.

\begin{example}
This is how a sample exploitation can be structured:
\begin{description}
\item [injection] --- a malicious request is sent to the vulnerable web application with the goal of corrupting all the responses sent to legitimate clients from that moment on. For instance, more than one releases of the popular \textsf{WordPress} blog application are vulnerable to injection attacks\footnote{http://secunia.com/advisories/23595} that allow an attacker to permanently include arbitrary content to the pages. Typically, such an arbitrary content is malicious code (e.g., JavaScript, VBSCrip, ActionScript, ActiveX) that, every time a legitimate user requests the infected page, executes on the client host.
\item [infection] --- Assuming that the compromised site is frequently accessed --- this might be the realistic case of the \textsf{WordPress}-powered \textsf{ZDNet} news blog\footnote{http://wordpress.org/showcase/zdnet/} --- a significant amount of clients visit it. Due to the high popularity of vulnerable browsers and plug-ins, the client may run \textsf{Internet Explorer} --- that is the most popular --- or an outdated release of \textsf{Firefox} on \textsf{Windows}. This create the perfect circumstances for the malicious page to successfully execute. In the best case, it may download a virus or a generic malware from a website under control of the attacker, so infecting the machine. In the worst case, this code may also exploit specific browser vulnerabilities and execute in privileged mode.
\item [control \& use] --- The malicious code just download installs and hides itself onto the victim's computer, which has just joined a botnet. As part of it, the client host can be remotely controlled by the attackers who can, for instance, rent it, use its bandwidth and computational power along with other computers to run a distributed \ac{DoS} attack. Also, the host can be used to automatically perform the same attacks described above against other vulnerable web applications. And so forth. \end{description}
\end{example}

This simple yet quite realistic example shows the various kinds of malicious activity that are generated during a typical drive-by exploitation. It also shows its requirements and assumptions that must hold to guarantee success. More precisely, we can recognize:

\begin{description}
\item[network activity] --- clearly, the whole interaction relies on a network connection over the Internet: the \ac{HTTP} connections used, for instance, to download the malicious code as well as to launch the injection attack used to compromise the web server.
\item[host activity] --- similarly to every other type of attack against an application, when the client-side code executes, the browser (or one of its extension plug-ins) is forced to behave improperly. If the malicious code executes till completion the attack succeeds and the host is infected. This happens only if the platform, operating system, and browser all match the requirements assumed by the exploit designer. For instance, the attack may succeed on \textsf{Windows} and not on \textsf{Mac OS X}, although the vulnerable version of, say, \textsf{Firefox} is the same on both the hosts.
\item[HTTP traffic] --- in order to exploit the vulnerability of the web application, the attacking client must generate malicious \ac{HTTP} requests. For instance, in the case of an \ac{SQL} injection --- that is the second most common vulnerability in a web application --- instead of a regular

\begin{logs}
GET /index.php?username=myuser
\end{logs}
  
\noindent the web server might be forced to process a

\begin{logs}
GET /index.php?username=' OR 'x'='x'--\&content=<script src="evil.com/code.js">
\end{logs}

\noindent that causes the \texttt{index.php} page to behave improperly.
\end{description}

It is now clear that protection mechanisms that analyze the network traffic, the activity of the client's operating system, the web server's \ac{HTTP} logs, or any combination of the three, have chances of recognizing that something malicious is happening in the network. For instance, if the \ac{ISP} network adopt \textsf{Snort}, a lightweight \ac{IDS} that analyzes the network traffic for known attack patterns, could block all the packets marked as suspicious. This would prevent, for instance, the \ac{SQL} injection to reach the web application. A similar protection level can be achieved by using other tools such as \textsf{ModSecurity} \citep{ristic:mod_security}. One of the problems that may arise with these classic, widely adopted solutions is if a zero day\index{0-day} attack is used. A zero day attack or threat exploits a vulnerability that is unknown to the public, undisclosed to the software vendor, or a fix is not available; thus, protection mechanisms that merely blacklist known malicious activity immediately become ineffective. In a similar vein, if the client is protected by an anti-virus, the infection phase can be blocked. However, this countermeasure is once again successful only if the anti-virus is capable of recognizing the malicious code, which assumes that the code is known to be malicious.

Ideally, an effective and comprehensive countermeasure can be achieved if all the protection tools involved (e.g., client-side, server-side, network-side) can collaborate together. For instance, if a website is publicly reported to be malicious, a client-side protection tool should block all the content downloaded from that particular website. This is only a simple example.

Thus, countermeasures against todays' threats already exist but are subject to at least two drawbacks:

\begin{itemize}
\item they offer protection only against known threats. To be effective we must assume that all the hostile traffic can be enumerated, which is clearly an impossible task.

\begin{quotation}
Why is ``Enumerating Badness'' a dumb idea? It's a dumb idea because sometime around 1992 the amount of Badness in the Internet began to vastly outweigh the amount of Goodness. For every harmless, legitimate, application, there are dozens or hundreds of pieces of malware, worm tests, exploits, or viral code. Examine a typical antivirus package and you'll see it knows about 75,000+ viruses that might infect your machine. Compare that to the legitimate 30 or so apps that I've installed on my machine, and you can see it's rather dumb to try to track 75,000 pieces of Badness when even a simpleton could track 30 pieces of Goodness~\citep{ranum-myths}.
\end{quotation}

\item they lack of cooperation, which is crucial to detect global and slow attacks.
\end{itemize}

This said, we conclude that classic approaches such as dynamic and static code analysis and \ac{IDS} already offer good protection but industry and research should move toward methods that require little or no knowledge. In this work, we indeed focus on the so called anomaly-based approaches, i.e., those that attempt to recognize the threats by detecting any variation from a system's normal operation, rather than looking for signs of known-to-be-malicious activity.

\section{Original Contributions}
\label{introduction:contributions} Our main research area is \ac{ID}. In particular, we focus on anomaly-based approaches to detect malicious activities. Since todays' threats are complex, a single point of inspection is not effective. A more comprehensive monitoring system is more desirable to protect both the network, the applications running on a certain host, and the web applications (that are particularly exposed due to the immense popularity of the Web). Our contributions focus on the mitigation of both host-based and web-based attacks, along with two techniques to correlate alerts from hybrid sensors.

\subsection{Host-based Anomaly Detection} Typical malicious processes can be detected by modeling the characteristics (e.g., type of arguments, sequences) of the system calls executed by the kernel, and by flagging unexpected deviations as attacks. Regarding this type of approaches, our contributions focus on hybrid models to accurately characterize the behavior of a binary application. In particular:

\begin{itemize}
\item we enhanced, re-engineered, and evaluated a novel tool for modeling the normal activity of the Linux 2.6 kernel. Compared to other existing solutions, our system shows better detection capabilities and good contextualization of the alerts reported.
\item We engineered and evaluated an \ac{IDS} to demonstrate that the combined use of (1) deterministic models to characterize a process' control flow and (2) stochastic models to capture normal features of the data flow, lead to better detection accuracy. Compared to the existing deterministic and stochastic approaches separately, our system shows better accuracy, with almost zero false positives.
\item We adapted our techniques for forensics investigation. By running experiments on real-world data and attacks, we show that our system is able to detect hidden tamper evidence although sophisticated anti-forensics tools (e.g., userland process execution) have been used.
\end{itemize}

\subsection{Web-based Anomaly Detection} Attempts of compromising a web application can be detected by modeling the characteristics (e.g., parameter values, character distributions, session content) of the \ac{HTTP}\index{HTTP} messages exchanged between servers and clients during normal operation. This approach can detect virtually any attempt of tampering with \ac{HTTP}\index{HTTP} messages, which is assumed to be evidence of attack. In this research field, our contributions focus on training data scarcity issues along with the problems that arise when an application changes its legit behavior. In particular:

\begin{itemize}
\item we contributed to the development of a system that learns the legit behavior of a web application. Such a behavior is defined by means of features extracted from 1) HTTP requests, 2) HTTP responses, 3) SQL queries to the underlying database, if any. Each feature is extracted and learned by using different models, some of which are improvements over well-known approaches and some others are original. The main contribution of this work is the \emph{combination} of database query models with HTTP-based models. The resulting system has been validated through preliminary experiments that shown very high accuracy.
\item we developed a technique to automatically detect legit changes in web applications with the goal of suppressing the large amount of false detections due to code upgrades, frequent in todays' web applications. We run experiments on real-world data to show that our simple but very effective approach accurately predict changes in web applications and can distinguish good \emph{vs.} malicious changes (i.e., attacks).
\item We designed and evaluated a machine learning technique to aggregate \ac{IDS} models with the goal of ensuring good detection accuracy even in case of scarce training data available. Our approach relies on clustering techniques and nearest-neighbor search to look-up well-trained models used to replace under-trained ones that are prone to overfitting and thus false detections. Experiments on real-world data have shown that almost every false alert due to overfitting is avoided with as low as 32-64 training samples per model.
\end{itemize}

Although these techniques have been developed on top of a web-based anomaly detector, they are sufficiently generic to be easily adapted to other systems using learning approaches.

\subsection{Alert Correlation} \ac{IDS} alerts are usually post-processed to generate compact reports and eliminate redundant, meaningless, or false detections. In this research field, our contributions focus on unsupervised techniques applied to aggregate and correlate alert events with the goal of reducing the effort of the security officer. In particular:

\begin{itemize}
\item We developed and tested an approach that accounts for the common measurement errors (e.g., delays and uncertainties) that occur in the alert generation process. Our approach exploits fuzzy metrics both to model errors and to construct an alert aggregation criterion based on distance in time. This technique has been show to be more robust compared to classic time-distance based aggregation metrics.
\item We designed and tested a prototype that models the alert generation process as a stochastic process. This setting allowed us to construct a simple, non-parametric hypothesis test that can detect whether two alert streams are correlated or not. Besides its simplicity, the advantage of our approach is to not requiring any parameter.
\end{itemize}

The aforementioned results have been published in the proceedings of international conferences and international journals.